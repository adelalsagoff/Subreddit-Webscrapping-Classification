{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "This notebook objective is to select the best model at predicting a subreddit post belonging to either 'r/GoogleHome' or 'r/AmazonEcho'.\n",
    "\n",
    "A dataset that had been further engineered from the EDA notebook will be used and would also undego further preprocessing using stemming and lemmatization. This processed data would then undergo a TFIDvectorizer transformer before training on Naive Bayes and Random Forest Models.\n",
    "\n",
    "Additionally, the best algorithm, based on accuracy, would be selected and would also be tested against data that does not include key words and phrases such as 'Google Home' and 'Amazon Echo' just to name a few. \n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Google's smart speaker system, Google Home, was designed to compete with the popular Amazon Echo. Both product serve as a vehicle to their respective voice-activated virtual helper that connects to the internet. \n",
    "\n",
    "Reddit users have used the platform as a forum to discuss their experience with the products including troubleshooting. I had been tasked by Google's Research team to analyze customer sentiment towards Google Home from subreddit posts on 'r/GoogleHome'.\n",
    "\n",
    "Additionally the Research Team would also like to compare customer pain points between their product and Amazon Echo, to improve the product itself. Therefore subreddit posts from 'r/AmazonEcho' would also be included in the dataset.\n",
    "\n",
    "A model (insert type of model) would also be built to predict if a given set of words do in fact refer to the discussion of either the Amazon Echo or the Google Home based on selected features. \n",
    "\n",
    "Each subreddit post are represented as 'documents' in the dataset and therefore both terms will be used interchangebly.\n",
    "\n",
    "**Contents**\n",
    "- [Import libraries](#Import-libraries)\n",
    "- [Load data](#Load-data)\n",
    "- [Preprocessing](#Preprocessing)\n",
    "- [Create multiple dataframes from different preprocessing methods](#Create-multiple-dataframes-fromdifferent-preprocessing-methods)\n",
    "- [Decision to only use Tfidvectorizer](#Decision-to-only-use-Tfidvectorizer)\n",
    "- [Modeling using lemmatized text](#Modeling-using-lemmatized-text)\n",
    "    - [Naive Bayes Model](#Naive-Bayes-Model)\n",
    "    - [Random Forest Model](#Random-Forest-Model)\n",
    "- [Modeling using stemmed text](#Modeling-using-lemmatized-text)\n",
    "    - [Naive Bayes Model](#Naive-Bayes-Model)\n",
    "    - [Random Forest Model](#Random-Forest-Model)\n",
    "- [Include additional features to see if model improves](#Include-additional-features-to-see-if-model-improves)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Recommendations](#Shortcomings-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing and Modeling</b>\n",
    "\n",
    "\n",
    "Does the student properly split and/or sample the data for validation/training purposes?\n",
    "\n",
    "Does the student defend their choice of production model relevant to the data at hand and the problem?\n",
    "\n",
    "Does the student explain how the model works and evaluate its performance successes/downfalls?\n",
    "\n",
    "<b>Evaluation and Conceptual Understanding</b>\n",
    "\n",
    "Does the student accurately identify and explain the baseline score?\n",
    "\n",
    "Does the student select and use metrics relevant to the problem objective?\n",
    "\n",
    "Accuracy will be used best metric to use here. Improperly classification in terms of false negative or false positive does not have dire consequences as opposed to fields such as healthcare. \n",
    "\n",
    "Does the student interpret the results of their model for purposes of inference?\n",
    "\n",
    "Is domain knowledge demonstrated when interpreting results?\n",
    "\n",
    "\n",
    "\n",
    "Does the student provide appropriate interpretation with regards to descriptive and inferential statistics?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_data = pd.read_csv(r'../datasets/modelling_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5733 entries, 0 to 5732\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sentiment_score  5733 non-null   float64\n",
      " 1   word_count       5733 non-null   int64  \n",
      " 2   linktype         5733 non-null   object \n",
      " 3   selftext_        5733 non-null   object \n",
      " 4   num_comments     5733 non-null   int64  \n",
      " 5   subreddit_       5733 non-null   object \n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 268.9+ KB\n"
     ]
    }
   ],
   "source": [
    "subreddit_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amazonecho    2968\n",
       "googlehome    2765\n",
       "Name: subreddit_, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_data.subreddit_.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change y_target to binary\n",
    "\n",
    "subreddit_data['subreddit_'] = subreddit_data['subreddit_'].apply(lambda x: 1 if x == 'googlehome' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2968\n",
       "1    2765\n",
       "Name: subreddit_, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if lambda function works \n",
    "\n",
    "subreddit_data.subreddit_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummify categorical columns\n",
    "linktype_dummy = pd.get_dummies(data=subreddit_data['linktype'],prefix='_',drop_first=True)\n",
    "\n",
    "#drop linktype data\n",
    "subreddit_data.drop(columns='linktype',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat new df\n",
    "subreddit_data = pd.concat([subreddit_data,linktype_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5733 entries, 0 to 5732\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sentiment_score  5733 non-null   float64\n",
      " 1   word_count       5733 non-null   int64  \n",
      " 2   selftext_        5733 non-null   object \n",
      " 3   num_comments     5733 non-null   int64  \n",
      " 4   subreddit_       5733 non-null   int64  \n",
      " 5   __link           5733 non-null   uint8  \n",
      " 6   __none           5733 non-null   uint8  \n",
      "dtypes: float64(1), int64(3), object(1), uint8(2)\n",
      "memory usage: 235.3+ KB\n"
     ]
    }
   ],
   "source": [
    "subreddit_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate text for comparison\n",
    "text_to_convert = subreddit_data['selftext_']\n",
    "\n",
    "#create new df without selftext as additional features to be trained on moving forward\n",
    "subreddit_data_notext = subreddit_data.drop(columns='selftext_',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5733 entries, 0 to 5732\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sentiment_score  5733 non-null   float64\n",
      " 1   word_count       5733 non-null   int64  \n",
      " 2   num_comments     5733 non-null   int64  \n",
      " 3   subreddit_       5733 non-null   int64  \n",
      " 4   __link           5733 non-null   uint8  \n",
      " 5   __none           5733 non-null   uint8  \n",
      "dtypes: float64(1), int64(3), uint8(2)\n",
      "memory usage: 190.5 KB\n"
     ]
    }
   ],
   "source": [
    "subreddit_data_notext.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set y target\n",
    "y = subreddit_data['subreddit_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multiple dataframes from different preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#create lemmatized text df\n",
    "lem_text = pd.DataFrame(subreddit_data['selftext_'])\n",
    "\n",
    "for row in range(0,len(subreddit_data)):\n",
    "    word_list = []\n",
    "    for word in subreddit_data.iat[row,2].split():\n",
    "        word_list.append(lemmatizer.lemmatize(word))\n",
    "    lem_text.iat[row,0] = (' ').join(word_list)\n",
    "\n",
    "#instantiate stemmer\n",
    "p_stem = PorterStemmer()\n",
    "\n",
    "#create stemmed text df\n",
    "stem_text = pd.DataFrame(subreddit_data['selftext_'])\n",
    "\n",
    "for row in range(0,len(subreddit_data)):\n",
    "    word_list = []\n",
    "    for word in subreddit_data.iat[row,2].split():\n",
    "        word_list.append(p_stem.stem(word))\n",
    "    stem_text.iat[row,0] = (' ').join(word_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before transformation:\n",
      "Issue with all speakers and displays. When I open in home app it just spins and doesn't load. After I tap a gear on top right for settings it tells me that I am not connected to same wifi but I absolutely am. Has anyone experienced this? Galaxy S9 and latest Home app\n",
      "\n",
      "\n",
      "Text after lemmatizer:\n",
      "Issue with all speaker and displays. When I open in home app it just spin and doesn't load. After I tap a gear on top right for setting it tell me that I am not connected to same wifi but I absolutely am. Has anyone experienced this? Galaxy S9 and latest Home app\n",
      "\n",
      "\n",
      "Text after stemmer:\n",
      "issu with all speaker and displays. when i open in home app it just spin and doesn't load. after i tap a gear on top right for set it tell me that i am not connect to same wifi but i absolut am. ha anyon experienc this? galaxi s9 and latest home app\n"
     ]
    }
   ],
   "source": [
    "#check if text had transformed accordingly\n",
    "\n",
    "print('Text before transformation:')\n",
    "print(f'{text_to_convert[2]}')\n",
    "print('\\n')\n",
    "print('Text after lemmatizer:')\n",
    "print(f'{lem_text.iat[2,0]}')\n",
    "print('\\n')\n",
    "print('Text after stemmer:')\n",
    "print(f'{stem_text.iat[2,0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All text data has gone through the apporiate preprocessng methods to test if they would yield different performance on the models. Moving forward, pipelines would be made for the respective models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.517705\n",
       "1    0.482295\n",
       "Name: subreddit_, dtype: float64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should our model were to predict every post to be a Google Home post, the model's accuracy score would be 0.4823. Other models will be compared against this score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision to only use Tfidvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidVectorizer was chosen as the only preprocessing method to prepare our text data for out models. This decision is because CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped in processing our data more efficiently.\n",
    "\n",
    "In TfidfVectorizer, it considers overall document weightage of a word. It helps  in dealing with most frequent words. Using it will penalize them and therefore have an effect on keywords such as 'Google' and 'Amazon'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling using lemmatized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn lemtext into \n",
    "X = lem_text.squeeze()\n",
    "\n",
    "#perform train test split for X_lem and y_target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state = 42)\n",
    "\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [2_000, 3_000, 4_000, 5_000], #number of max feature\n",
    "    'tvec__stop_words': [None, 'english'],  #with or without stopword\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]   #word or bigram\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_tvec = GridSearchCV(pipe_tvec, \n",
    "                        param_grid = pipe_tvec_params, \n",
    "                        cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_features': 5000,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9530123284484764"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940027894002789"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tvec.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             \t| Lemmetized  \t|\n",
    "|-------------\t|-------------\t|\n",
    "|             \t| Naive Bayes \t|\n",
    "| train score \t| 0.9530      \t|\n",
    "| test score  \t| 0.8940      \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Base model returned relatively good scores and does not appear to have overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instsantiate Tfid\n",
    "tvec = TfidfVectorizer(max_features=5_000,stop_words='english')\n",
    "\n",
    "#transform lemm text\n",
    "X_lemm_tfid = pd.DataFrame(tvec.fit_transform(lem_text.squeeze()).todense(), \n",
    "                          columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lemm_tfid,y, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 1, 2, 3],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "rf_gs = GridSearchCV(rf,\n",
    "                     param_grid=rf_params,\n",
    "                     cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 1, 2, 3],\n",
       "                         'n_estimators': [100, 150]})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'n_estimators': 100}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967434287043498"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900278940027894"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             \t| Lemmetized  \t| Lemmetized    \t|\n",
    "|-------------\t|-------------\t|---------------\t|\n",
    "|             \t| Naive Bayes \t| Random Forest \t|\n",
    "| train score \t| 0.9530      \t| 0.9967        \t|\n",
    "| test score  \t| 0.8940      \t| 0.9002        \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest had beat both the train accuracy score and train test score. Random Forest model shows overfitting from the train accuracy score even with a grid search, having 100 trees and potentially fitting to noise in the data. Irregardless we would still move forward and see how the Random Forrest fare against stemmed text.\n",
    "\n",
    "Random Forest appears to be better predictive algorithm at the moment. \n",
    "\n",
    "Next, we will try the two algorithms on stemmed text data instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling using stemmed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn lemtext into series\n",
    "X = stem_text.squeeze()\n",
    "\n",
    "#perform train test split for X_stem and y_target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state = 42)\n",
    "\n",
    "pipe_stem_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_stem_tvec_params = {\n",
    "    'tvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_stem_tvec = GridSearchCV(pipe_tvec, \n",
    "                        param_grid = pipe_tvec_params, \n",
    "                        cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_stem_tvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_features': 5000,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_stem_tvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544080018608979"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_stem_tvec.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.897489539748954"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_stem_tvec.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             \t| Lemmetized  \t| Lemmetized    \t| Stemmed     \t|\n",
    "|-------------\t|-------------\t|---------------\t|-------------\t|\n",
    "|             \t| Naive Bayes \t| Random Forest \t| Naive Bayes \t|\n",
    "| train score \t| 0.9530      \t| 0.9967        \t| 0.9544      \t|\n",
    "| test score  \t| 0.8940      \t| 0.9002        \t| 0.8975      \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes model returned better results with stemmed text data compared to lemmetized text yet still underperformed compared to the previous Random Forest model. We will see how the Random Forest algorithm performs with stemmed text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instsantiate Tfid\n",
    "tvec = TfidfVectorizer(max_features=5_000,stop_words='english')\n",
    "\n",
    "#transform lemm text\n",
    "X_stem_tfid = pd.DataFrame(tvec.fit_transform(stem_text.squeeze()).todense(), \n",
    "                          columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_stem_tfid,y, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 1, 2, 3],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "rf_gs = GridSearchCV(rf,\n",
    "                     param_grid=rf_params,\n",
    "                     cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 1, 2, 3],\n",
       "                         'n_estimators': [100, 150]})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'n_estimators': 150}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965108164689462"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9037656903765691"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             \t| Lemmetized  \t| Lemmetized    \t| Stemmed     \t| Stemmed       \t|\n",
    "|-------------\t|-------------\t|---------------\t|-------------\t|---------------\t|\n",
    "|             \t| Naive Bayes \t| Random Forest \t| Naive Bayes \t| Random Forest \t|\n",
    "| train score \t| 0.9530      \t| 0.9967        \t| 0.9544      \t| 0.9965        \t|\n",
    "| test score  \t| 0.8940      \t| 0.9002        \t| 0.8975      \t| 0.9038        \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model on stemmed data outperformed all models in accuracy test scores. Additionally, Random Forest does seem to be overfitting again, even with more trees, 150.\n",
    "\n",
    "However upon closer inspection, te Random Forest model had reduced fitting by 0.0002 in terms of score, with 50 more trees, when in theory the tendency to overfitting should decrease. However it would not make much difference.\n",
    "\n",
    "Ultimately, it is safe to say that the Random Forest algorithm performs the best, along with stemming as the text data preprocessing method of choice. \n",
    "\n",
    "Next we shall use Random Forest against text data that does not have obvious keywords like 'Google' and 'Amazon' etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Random Forest algorithm on data without keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of additional words to be including in TFID stop words\n",
    "additional_words = ['google','amazon','mini','dot','hub','max','echo','alexa','assistant','nest','amp','x200b']\n",
    "\n",
    "stop_words = set(text.ENGLISH_STOP_WORDS.union(additional_words))\n",
    "\n",
    "# Instantiate the transformer for text data with no key words(nkw)\n",
    "tvec_nkw = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "#put converted data into respective dataframe\n",
    "tvec_nkw_df = pd.DataFrame(tvec_nkw.fit_transform(stem_text.squeeze()).todense(), \n",
    "                          columns=tvec_nkw.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tvec_nkw_df,y, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 1, 2, 3],\n",
       "                         'n_estimators': [100, 150]})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_nkw = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 1, 2, 3],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "rf_gs = GridSearchCV(rf,\n",
    "                     param_grid=rf_params,\n",
    "                     cv=5)\n",
    "\n",
    "rf_gs_nkw.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'n_estimators': 150}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs_nkw.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model accuracy score with training data: 0.9988369388229821\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Model accuracy score with training data: {rf_gs_nkw.score(X_train,y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model accuracy score with test data: 0.8068340306834031\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest Model accuracy score with test data: {rf_gs_nkw.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             \t| with keywords \t| without keywords \t|\n",
    "|-------------\t|---------------\t|------------------\t|\n",
    "|             \t| Random Forest \t| Random Forest    \t|\n",
    "| train score \t| 0.9965        \t| 0.9988           \t|\n",
    "| test score  \t| 0.9024        \t| 0.8068           \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the algorithm had performed worse with text data without key words, with 150 trees. A workaround to improve this model is to include additional features that were previous explored in our EDA, such as number of comments, link types and word counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include additional features to see if model improves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randon Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer for text data with no key words(nkw)\n",
    "tvec_nkw = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "#put converted data into respective dataframe\n",
    "tvec_nkw_df = pd.DataFrame(tvec_nkw.fit_transform(stem_text.squeeze()).todense(), \n",
    "                          columns=tvec_nkw.get_feature_names())\n",
    "\n",
    "#cocat additional data with tfid df with no keyword\n",
    "tvec_nkw_df = pd.concat([tvec_nkw_df,subreddit_data_notext],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tvec_nkw_df,y, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forrest with no keywords and added features\n",
    "rf_nkf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 1, 2, 3],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "gs_rf_nkf = GridSearchCV(rf_nkf, \n",
    "                        param_grid = rf_params,\n",
    "                            cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 1, 2, 3],\n",
       "                         'n_estimators': [100, 150]})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf_nkf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'n_estimators': 150}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf_nkf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf_nkf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99860529986053"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf_nkf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             \t| with keywords \t| without keywords \t| without keywords and additional features \t|\n",
    "|-------------\t|---------------\t|------------------\t|------------------------------------------\t|\n",
    "|             \t| Random Forest \t| Random Forest    \t| Random Forest                            \t|\n",
    "| train score \t| 0.9965        \t| 0.9988           \t| 1.0|\n",
    "| test score  \t| 0.9024        \t| 0.8068           \t| 0.9986                                   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Random Forest Model, with the same parameters ie. 150 trees, performed significantly better with almost a perfect score.\n",
    "\n",
    "What can be concluded is that the features added into our model was extremely useful in the classification of the post to the right subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming our text data had resulted in better accuracy score across our Naive Bayes and Random Forest model. This is likely due to the nature of our problem, where we are simply classifying which post belonged to which subreddit. The application of lemmatization is better suited for more complex task such as building a language application where sesitivity to language inflections is important, which also may have been the reason for lower accuracy score. \n",
    "\n",
    "Our Random Forest model had also performed better than Naive Bayes overall. A possible reason would be the conditions the Naive Bayes model requires for it to work well. Naive Bayes works best when you have small training data set and relatively small features(dimensions). Since we have very high dimensionaly, the model may not give you accuracy, because the likelihood would be distributed and may not follow the Gaussian or other distribution. Another condition for Naive Bayes to work is that features should be dependent of each other and with text data, there is a likelihood that certain words are dependent of each other, for example bigrams or trigrams. \n",
    "\n",
    "Lastly, our Random Forest performed better with additional features, to the point wher even in our test score, we were close to perfect predictions. This is representative of how with more data, lowers estimation variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model had been trained on data that have been cleaned and preprocessed. Post that were regarded as 'spam' post were removed, in order to ensure accurate predictions. Which had been 7% of the initial data scraped from the Google Home subreddit. The models tested had not been trained on these types of posts, therefore the model may do poorly when prediciting the right subreddit when fed posts like these. \n",
    "\n",
    "Training the model with spam post may yield different results and would be interesting to test. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
