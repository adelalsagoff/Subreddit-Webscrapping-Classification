{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extract, Transform, Load\n",
    "\n",
    "This notebook objective is to scrape 2 subreddit websites, followed by transforming the unstructured data through 2 different prepocessing techniques, and export the cleaned and structured data for exploratory analysis.  \n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Google's smart speaker system, Google Home, was designed to compete with the popular Amazon Echo. Both product serve as a vehicle to their respective voice-activated virtual helper that connects to the internet. \n",
    "\n",
    "Reddit users have used the platform as a forum to discuss their experience with the products. I had been tasked by Google's Research team to analyze customer sentiment towards Google Home from subreddit posts on 'r/GoogleHome'.\n",
    "\n",
    "Additionally the Research Team would also like to find out what common and unique customer pain points are prevalent between the Google Home and Amazon Echo, with the goal of designing a better product. Therefore subreddit posts from 'r/AmazonEcho' would also be included in the dataset.\n",
    "\n",
    "A Random Forrest model would also be built to predict if a given set of words do in fact refer to the discussion of either the Amazon Echo or the Google Home based on selected features. \n",
    "\n",
    "Each subreddit post are represented as 'documents' in the dataset and therefore both terms will be used interchangebly.\n",
    "\n",
    "\n",
    "**Contents**\n",
    "- [Import libraries](#Import-libraries)\n",
    "- [Data collection via webscrapping](#Data-collection-via-webscrapping)\n",
    "- [Data cleaning](#Data-cleaning)\n",
    "    - [investigate on documents with missing selftext](#investigate-on-documents-with-missing-selftext)\n",
    "    - [Investigate for reoccuring urls](#Investigate-for-reoccuring-urls)\n",
    "    - [Investigate posts made by same authors](#Investigate-posts-made-by-same-authors)\n",
    "    - [Dropping unwanted documents](#Dropping-unwanted-documents)\n",
    "- [Feature Selction and Engineering](#Feature-Selction-and-Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection via webscrapping\n",
    "#### *Only run notebook code in the next section. This section is to only illustrate webscrapping codeblocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Using Push Shift api to webscrape\n",
    "# through subreddit r/GoogleHome and r/AmazonEcho\"\"\"\n",
    "\n",
    "# assign url for request\n",
    "# url =  'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"using a while loop to iterate \n",
    "# through posts from different utc values\n",
    "# and break when the dataframe has reached \n",
    "# desired length\"\"\"\n",
    "\n",
    "# ggl_params = {\n",
    "#     'subreddit': 'googlehome',\n",
    "#     'size': 100,\n",
    "#     'before': None\n",
    "# }\n",
    "\n",
    "# ggl_df = pd.DataFrame()   #created empty df to be concated to and while loop bool\n",
    "\n",
    "# while len(ggl_df) < 3000:\n",
    "#     res = requests.get(url,ggl_params)              #create request\n",
    "#     data = res.json()['data']                       #extract j.son data\n",
    "#     data = pd.DataFrame(data)                       #create extrated data into df\n",
    "#     ggl_df = pd.concat([ggl_df,data],axis=0)         # concat new df with old df\n",
    "#     ggl_df.reset_index(drop=True,inplace=True)          #reset df index\n",
    "#     ggl_params['before'] = ggl_df.created_utc[len(ggl_df)-1] #change params\n",
    "    \n",
    "    \n",
    "# print(ggl_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"using a while loop to iterate \n",
    "# through posts from different utc values\n",
    "# and break when the dataframe has reached \n",
    "# desired length\"\"\"\n",
    "\n",
    "# amz_params = {\n",
    "#     'subreddit': 'amazonecho',\n",
    "#     'size': 100,\n",
    "#     'before': None\n",
    "# }\n",
    "\n",
    "# amz_df = pd.DataFrame()   #created empty df to be concated to and while loop bool\n",
    "\n",
    "# while len(amz_df) < 3000:\n",
    "#     res = requests.get(url,amz_params)              #create request\n",
    "#     data = res.json()['data']                       #extract j.son data\n",
    "#     data = pd.DataFrame(data)                       #create extrated data into df\n",
    "#     amz_df = pd.concat([amz_df,data],axis=0)         # concat new df with old df\n",
    "#     amz_df.reset_index(drop=True,inplace=True)          #reset df index\n",
    "#     amz_params['before'] = amz_df.created_utc[len(amz_df)-1] #change params\n",
    "    \n",
    "    \n",
    "# print(amz_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export unstructured data\n",
    "# ggl_df.to_csv(r'../datasets/unclean_google_data.csv')\n",
    "# amz_df.to_csv(r'../datasets/unclean_amazon_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "#### *run code starting from this section of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggl_df = pd.read_csv('../datasets/unclean_google_data.csv')\n",
    "amz_df = pd.read_csv('../datasets/unclean_amazon_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Home df shape:(3000, 85)\n",
      "Amazon Echo df shape:(3000, 83)\n"
     ]
    }
   ],
   "source": [
    "#check shape of dfs\n",
    "\n",
    "print(f'Google Home df shape:{ggl_df.shape}')\n",
    "print(f'Amazon Echo df shape:{amz_df.shape}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicates in Goggle Home df: 0\n",
      "number of duplicates in Amazon echo df: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"sanity check to see if scraping was done correctly\n",
    "through checking of dupplicates using title and utc\"\"\"\n",
    "\n",
    "print(f\"number of duplicates in Goggle Home df: {ggl_df[ggl_df.duplicated(subset=('title','created_utc'))].shape[0]}\")\n",
    "\n",
    "print(f\"number of duplicates in Amazon echo df: {amz_df[amz_df.duplicated(subset=('title','created_utc'))].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicates for self text in Goggle Home df: 918\n",
      "number of duplicates for self text in Amazon echo df: 765\n"
     ]
    }
   ],
   "source": [
    "#secondary sanity check for duplicates on self text\n",
    "\n",
    "print(f\"number of duplicates for self text in Goggle Home df: {ggl_df[ggl_df.duplicated(subset = 'selftext')].shape[0]}\")\n",
    "\n",
    "print(f\"number of duplicates for self text in Amazon echo df: {amz_df[amz_df.duplicated(subset='selftext')].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     NaN\n",
       "14    NaN\n",
       "28    NaN\n",
       "38    NaN\n",
       "43    NaN\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#further investigation of Google Home df self.text duplicates\n",
    "\n",
    "ggl_df[ggl_df.duplicated(subset = 'selftext')]['selftext'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3            Trouble enabling Voice Match for 2nd user?\n",
       "14    Project CHIP Finally coming end of 2021, wonde...\n",
       "28    Samsung Range not able to connect to Google Ho...\n",
       "38    Buy Google 5 Star Reviews - 100% Safe &amp; Pe...\n",
       "43    How to Trigger Routines at Sunrise/Sunset on G...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to see corresponding titles of documents with empty self text\n",
    "\n",
    "ggl_df[ggl_df.duplicated(subset = 'selftext')]['title'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    NaN\n",
       "17    NaN\n",
       "20    NaN\n",
       "27    NaN\n",
       "30    NaN\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat same steps with Amazon Echo df\n",
    "\n",
    "amz_df[amz_df.duplicated(subset = 'selftext')]['selftext'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13                                                Day 9\n",
       "17    $20 off All-new Echo Buds (2nd Gen) for pre-or...\n",
       "20    23% off Echo Show 8 HD smart display with Alex...\n",
       "27    My wife prefers a specific temperature inside ...\n",
       "30      Amazon announces new Echo Buds with ANC feature\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to see corresponding titles of documents with empty self text\n",
    "\n",
    "amz_df[amz_df.duplicated(subset = 'selftext')]['title'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b><br/>\n",
    "917 documents in the Ggl_df and 765 documents in Amz_df are posts that had titles but no self text. \n",
    "\n",
    "Potential quesitons to answer are as follows:\n",
    "\n",
    "Are posts that are solely headings useful data?<br/>\n",
    "Should posts that are solely titles be feature engineered as self text?<br/>\n",
    "Do these posts tend to include comments, or URL or other similarities?<br/>\n",
    "\n",
    "We want to ensure that these post would not negatively affect the predictive power of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate documents with missing selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dfs of documents with only titles to investigate usefulness\n",
    "\n",
    "ggl_titles_only = ggl_df[ggl_df.duplicated(subset = 'selftext')].index.tolist() #put index of documents with titles only into list\n",
    "ggl_title_only_df = ggl_df.iloc[ggl_titles_only,:]    #create df of documents with titles only\n",
    "\n",
    "amz_titles_only = amz_df[amz_df.duplicated(subset = 'selftext')].index.tolist() #put index of documents with titles only into list\n",
    "amz_title_only_df = amz_df.iloc[amz_titles_only,:]    #create df of documents with titles only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help                           125\n",
      "Bug                             43\n",
      "Other                           40\n",
      "Tips                            20\n",
      "News                            18\n",
      "Hacks                            8\n",
      "NSFW - Language                  8\n",
      "Features WishList                8\n",
      "Product Review                   7\n",
      "Deals | Sales | Promotions       7\n",
      "Commands | How To's              7\n",
      "GH Trigger Warning               3\n",
      "Name: link_flair_text, dtype: int64\n",
      "\n",
      "\n",
      "Number of Ggl documents that are solely titles without a link_flair_text category: 624\n"
     ]
    }
   ],
   "source": [
    "\"\"\"refering to the data dectionary extracted from the j.son format of the subreddit websites,\n",
    "post are also categorized based on the type of posts, 'link_flair_text'\"\"\"\n",
    "\n",
    "print(ggl_title_only_df.link_flair_text.value_counts())\n",
    "print('\\n')\n",
    "print(f'Number of Ggl documents that are solely titles without a link_flair_text category: {len(ggl_title_only_df)-ggl_title_only_df.link_flair_text.value_counts().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question           158\n",
      "Review              72\n",
      "Technical Issue     63\n",
      "Feature             32\n",
      "Alexa Skill         31\n",
      "Feature Request     18\n",
      "Easter Egg           2\n",
      "Skill Request        1\n",
      "Name: link_flair_text, dtype: int64\n",
      "\n",
      "\n",
      "Number of Amz documents that are solely titles without a link_flair_text category: 388\n"
     ]
    }
   ],
   "source": [
    "print(amz_title_only_df.link_flair_text.value_counts())\n",
    "print('\\n')\n",
    "print(f'Number of Amz documents that are solely titles without a link_flair_text category: {len(amz_title_only_df)-(amz_title_only_df.link_flair_text.value_counts()).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "\n",
    "More than half of each subreddit dataframe has a link_flair_text category. Based on their category name, for example 'Question' and 'Bug', are more than likely to be posts that authors feel it would be redundant to elaborate in 'selftext', as just the titles would be self explanatory or instead of text, an image or url was used. \n",
    "\n",
    "These post would also likely be requesting for assistance or interaction in the form of comments. this inference can be supported by comparing the distrubution of comments or the presence of an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check distribution of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    294.000000\n",
       "mean      10.125850\n",
       "std       22.619296\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        2.000000\n",
       "75%        9.000000\n",
       "max      185.000000\n",
       "Name: num_comments, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of comments for only headings documents with a link_flair_text category\n",
    "\n",
    "ggl_title_only_df[ggl_title_only_df.link_flair_text.notnull()].num_comments.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>:\n",
    "Average of comments is 10 per document however this is likely skewed from a post with 185 comments. Further investigation is needed on this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': {1242: 1242},\n",
       " 'all_awardings': {1242: \"[{'award_sub_type': 'GLOBAL', 'award_type': 'global', 'awardings_required_to_grant_benefits': None, 'coin_price': 80, 'coin_reward': 0, 'count': 1, 'days_of_drip_extension': 0, 'days_of_premium': 0, 'description': 'Everything is better with a good hug', 'end_date': None, 'giver_coin_reward': 0, 'icon_format': 'PNG', 'icon_height': 2048, 'icon_url': 'https://i.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png', 'icon_width': 2048, 'id': 'award_8352bdff-3e03-4189-8a08-82501dd8f835', 'is_enabled': True, 'is_new': False, 'name': 'Hugz', 'penny_donate': 0, 'penny_price': 0, 'resized_icons': [{'height': 16, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=16&amp;height=16&amp;auto=webp&amp;s=73a23bf7f08b633508dedf457f2704c522b94a04', 'width': 16}, {'height': 32, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=32&amp;height=32&amp;auto=webp&amp;s=50f2f16e71d2929e3d7275060af3ad6b851dbfb1', 'width': 32}, {'height': 48, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=48&amp;height=48&amp;auto=webp&amp;s=ca487311563425e195699a4d7e4c57a98cbfde8b', 'width': 48}, {'height': 64, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=64&amp;height=64&amp;auto=webp&amp;s=7b4eedcffb1c09a826e7837532c52979760f1d2b', 'width': 64}, {'height': 128, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/ks45ij6w05f61_oldHugz.png?width=128&amp;height=128&amp;auto=webp&amp;s=e4d5ab237eb71a9f02bb3bf9ad5ee43741918d6c', 'width': 128}], 'resized_static_icons': [{'height': 16, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=16&amp;height=16&amp;auto=webp&amp;s=69997ace3ef4ffc099b81d774c2c8f1530602875', 'width': 16}, {'height': 32, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=32&amp;height=32&amp;auto=webp&amp;s=e9519d1999ef9dce5c8a9f59369cb92f52d95319', 'width': 32}, {'height': 48, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=48&amp;height=48&amp;auto=webp&amp;s=f076c6434fb2d2f9075991810fd845c40fa73fc6', 'width': 48}, {'height': 64, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=64&amp;height=64&amp;auto=webp&amp;s=85527145e0c4b754306a30df29e584fd16187636', 'width': 64}, {'height': 128, 'url': 'https://preview.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png?width=128&amp;height=128&amp;auto=webp&amp;s=b8843cdf82c3b741d7af057c14076dcd2621e811', 'width': 128}], 'start_date': None, 'static_icon_height': 2048, 'static_icon_url': 'https://i.redd.it/award_images/t5_q0gj4/fpm0r5ryq1361_PolarHugs.png', 'static_icon_width': 2048, 'subreddit_coin_reward': 0, 'subreddit_id': None, 'tiers_by_required_awardings': None}]\"},\n",
       " 'allow_live_comments': {1242: True},\n",
       " 'author': {1242: 'deathbygrugru'},\n",
       " 'author_flair_css_class': {1242: nan},\n",
       " 'author_flair_richtext': {1242: '[]'},\n",
       " 'author_flair_text': {1242: nan},\n",
       " 'author_flair_type': {1242: 'text'},\n",
       " 'author_fullname': {1242: 't2_7fdmxquu'},\n",
       " 'author_patreon_flair': {1242: False},\n",
       " 'author_premium': {1242: False},\n",
       " 'awarders': {1242: '[]'},\n",
       " 'can_mod_post': {1242: False},\n",
       " 'contest_mode': {1242: False},\n",
       " 'created_utc': {1242: 1614220587},\n",
       " 'domain': {1242: 'i.imgur.com'},\n",
       " 'full_link': {1242: 'https://www.reddit.com/r/googlehome/comments/lrvlop/finally_found_one_of_those_crazy_target_deals_had/'},\n",
       " 'gildings': {1242: '{}'},\n",
       " 'id': {1242: 'lrvlop'},\n",
       " 'is_crosspostable': {1242: True},\n",
       " 'is_meta': {1242: False},\n",
       " 'is_original_content': {1242: False},\n",
       " 'is_reddit_media_domain': {1242: False},\n",
       " 'is_robot_indexable': {1242: True},\n",
       " 'is_self': {1242: False},\n",
       " 'is_video': {1242: False},\n",
       " 'link_flair_background_color': {1242: '#dadada'},\n",
       " 'link_flair_richtext': {1242: '[]'},\n",
       " 'link_flair_text_color': {1242: 'dark'},\n",
       " 'link_flair_type': {1242: 'text'},\n",
       " 'locked': {1242: False},\n",
       " 'media_only': {1242: False},\n",
       " 'no_follow': {1242: False},\n",
       " 'num_comments': {1242: 185},\n",
       " 'num_crossposts': {1242: 0},\n",
       " 'over_18': {1242: False},\n",
       " 'parent_whitelist_status': {1242: 'all_ads'},\n",
       " 'permalink': {1242: '/r/googlehome/comments/lrvlop/finally_found_one_of_those_crazy_target_deals_had/'},\n",
       " 'pinned': {1242: False},\n",
       " 'pwls': {1242: 6},\n",
       " 'retrieved_on': {1242: 1614351570},\n",
       " 'score': {1242: 640},\n",
       " 'selftext': {1242: nan},\n",
       " 'send_replies': {1242: True},\n",
       " 'spoiler': {1242: False},\n",
       " 'stickied': {1242: False},\n",
       " 'subreddit': {1242: 'googlehome'},\n",
       " 'subreddit_id': {1242: 't5_3enp4'},\n",
       " 'subreddit_subscribers': {1242: 339978},\n",
       " 'subreddit_type': {1242: 'public'},\n",
       " 'thumbnail': {1242: 'https://b.thumbs.redditmedia.com/fUXIOSTPPPEcoQuE15Aa76yujfJPFXbW-3Y4hqlJJ7o.jpg'},\n",
       " 'title': {1242: 'Finally found one of those crazy target deals! Had to pick them up'},\n",
       " 'total_awards_received': {1242: 1},\n",
       " 'treatment_tags': {1242: '[]'},\n",
       " 'upvote_ratio': {1242: 0.97},\n",
       " 'url': {1242: 'https://i.imgur.com/tREFYrM.jpg'},\n",
       " 'whitelist_status': {1242: 'all_ads'},\n",
       " 'wls': {1242: 6},\n",
       " 'link_flair_template_id': {1242: 'dad079e8-d6c3-11e7-96b4-0e468bcaecf2'},\n",
       " 'link_flair_text': {1242: 'Deals | Sales | Promotions '},\n",
       " 'post_hint': {1242: 'image'},\n",
       " 'preview': {1242: \"{'enabled': True, 'images': [{'id': 'Ke5EhzxiVc7ZuZGZIURjl-N7-UJCJwIy6ADILyfN610', 'resolutions': [{'height': 144, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1caecb0746b3bdc9a083b16f855df8576b9b13c5', 'width': 108}, {'height': 288, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23dc57b0995ab1da28891b2bc2ef65afb14105e4', 'width': 216}, {'height': 426, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3f45a214bfb704d10dded5803c4093f1dd06ce9', 'width': 320}, {'height': 853, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4582c59649d9e54ce771b63e2f6eb5d06fced62', 'width': 640}, {'height': 1280, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a94e2cdcf77e48b46f55f8470d43a62489aa37f', 'width': 960}, {'height': 1440, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4b240e684a1ef22ec13a78dba6147dada19d621', 'width': 1080}], 'source': {'height': 2000, 'url': 'https://external-preview.redd.it/d9w3pehLSbKfbpv0BkoucImSHraM49DIRGffQgGGHbM.jpg?auto=webp&amp;s=a174367390f8641b7f41667ba2966b0edd22b2b9', 'width': 1500}, 'variants': {}}]}\"},\n",
       " 'thumbnail_height': {1242: 140.0},\n",
       " 'thumbnail_width': {1242: 140.0},\n",
       " 'url_overridden_by_dest': {1242: 'https://i.imgur.com/tREFYrM.jpg'},\n",
       " 'removed_by_category': {1242: nan},\n",
       " 'author_flair_background_color': {1242: nan},\n",
       " 'author_flair_text_color': {1242: nan},\n",
       " 'media': {1242: nan},\n",
       " 'media_embed': {1242: nan},\n",
       " 'secure_media': {1242: nan},\n",
       " 'secure_media_embed': {1242: nan},\n",
       " 'gallery_data': {1242: nan},\n",
       " 'is_gallery': {1242: nan},\n",
       " 'media_metadata': {1242: nan},\n",
       " 'author_flair_template_id': {1242: nan},\n",
       " 'author_cakeday': {1242: nan},\n",
       " 'poll_data': {1242: nan},\n",
       " 'crosspost_parent': {1242: nan},\n",
       " 'crosspost_parent_list': {1242: nan},\n",
       " 'edited': {1242: nan},\n",
       " 'collections': {1242: nan},\n",
       " 'banned_by': {1242: nan},\n",
       " 'gilded': {1242: nan},\n",
       " 'distinguished': {1242: nan}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data dictionary of document with 185 comments\n",
    "\n",
    "ggl_title_only_df[ggl_title_only_df.num_comments == 185].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to title only post, with a link_flair_text category, with highest number of comments: https://www.reddit.com/r/googlehome/comments/lrvlop/finally_found_one_of_those_crazy_target_deals_had/\n"
     ]
    }
   ],
   "source": [
    "#print url of post for further investigation\n",
    "print (f\"Link to title only post, with a link_flair_text category, with highest number of comments: {ggl_title_only_df[ggl_title_only_df.num_comments == 185].to_dict()['full_link'][1242]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>: Upon further investigation the document, this post had gone 'viral', an outlier occurence where many other redittors had interacted with this post.\n",
    "    \n",
    "The title of this post being 'Finally found one of those crazy target deals! Had to pick them up'. Followed by an image of a receipt where the redditor had purchased the Google Home at a low price.\n",
    "    \n",
    "This is also a good example of how just the heading alone conveys the overall sentiment of what the redditors wants to post without selftext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    624.000000\n",
       "mean       5.190705\n",
       "std       19.605021\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        2.000000\n",
       "max      221.000000\n",
       "Name: num_comments, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of comments for only headings documents without a link_flair_text category\n",
    "\n",
    "\"\"\"use link_flair_text.isnull() for documents without link_flair_text.isnull()\"\"\" \n",
    "ggl_title_only_df[ggl_title_only_df.link_flair_text.isnull()].num_comments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Home df shape:(3000, 85)\n",
      "Amazon Echo df shape:(3000, 83)\n"
     ]
    }
   ],
   "source": [
    "print(f'Google Home df shape:{ggl_df.shape}')\n",
    "print(f'Amazon Echo df shape:{amz_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>:\n",
    "Average of comments is 5 per document however this is likely skewed from a post with 221 comments. Further investigation is needed on this document. The median and 25 percentile is 0 highlighting barely any interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': {1206: 1206},\n",
       " 'all_awardings': {1206: \"[{'award_sub_type': 'GLOBAL', 'award_type': 'global', 'awardings_required_to_grant_benefits': None, 'coin_price': 125, 'coin_reward': 0, 'count': 1, 'days_of_drip_extension': 0, 'days_of_premium': 0, 'description': 'When you come across a feel-good thing.', 'end_date': None, 'giver_coin_reward': None, 'icon_format': None, 'icon_height': 2048, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'icon_width': 2048, 'id': 'award_5f123e3d-4f48-42f4-9c11-e98b566d5897', 'is_enabled': True, 'is_new': False, 'name': 'Wholesome', 'penny_donate': None, 'penny_price': None, 'resized_icons': [{'height': 16, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16}, {'height': 32, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32}, {'height': 48, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48}, {'height': 64, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64}, {'height': 128, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128}], 'resized_static_icons': [{'height': 16, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0', 'width': 16}, {'height': 32, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef', 'width': 32}, {'height': 48, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63', 'width': 48}, {'height': 64, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb', 'width': 64}, {'height': 128, 'url': 'https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b', 'width': 128}], 'start_date': None, 'static_icon_height': 2048, 'static_icon_url': 'https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png', 'static_icon_width': 2048, 'subreddit_coin_reward': 0, 'subreddit_id': None, 'tiers_by_required_awardings': None}]\"},\n",
       " 'allow_live_comments': {1206: False},\n",
       " 'author': {1206: '2tuff4u2'},\n",
       " 'author_flair_css_class': {1206: nan},\n",
       " 'author_flair_richtext': {1206: '[]'},\n",
       " 'author_flair_text': {1206: nan},\n",
       " 'author_flair_type': {1206: 'text'},\n",
       " 'author_fullname': {1206: 't2_q648wkk'},\n",
       " 'author_patreon_flair': {1206: False},\n",
       " 'author_premium': {1206: False},\n",
       " 'awarders': {1206: '[]'},\n",
       " 'can_mod_post': {1206: False},\n",
       " 'contest_mode': {1206: False},\n",
       " 'created_utc': {1206: 1614294726},\n",
       " 'domain': {1206: 'androidpolice.com'},\n",
       " 'full_link': {1206: 'https://www.reddit.com/r/googlehome/comments/lsjp5n/google_kills_most_of_its_disney_readalong_books/'},\n",
       " 'gildings': {1206: '{}'},\n",
       " 'id': {1206: 'lsjp5n'},\n",
       " 'is_crosspostable': {1206: True},\n",
       " 'is_meta': {1206: False},\n",
       " 'is_original_content': {1206: False},\n",
       " 'is_reddit_media_domain': {1206: False},\n",
       " 'is_robot_indexable': {1206: True},\n",
       " 'is_self': {1206: False},\n",
       " 'is_video': {1206: False},\n",
       " 'link_flair_background_color': {1206: nan},\n",
       " 'link_flair_richtext': {1206: '[]'},\n",
       " 'link_flair_text_color': {1206: 'dark'},\n",
       " 'link_flair_type': {1206: 'text'},\n",
       " 'locked': {1206: False},\n",
       " 'media_only': {1206: False},\n",
       " 'no_follow': {1206: False},\n",
       " 'num_comments': {1206: 221},\n",
       " 'num_crossposts': {1206: 0},\n",
       " 'over_18': {1206: False},\n",
       " 'parent_whitelist_status': {1206: 'all_ads'},\n",
       " 'permalink': {1206: '/r/googlehome/comments/lsjp5n/google_kills_most_of_its_disney_readalong_books/'},\n",
       " 'pinned': {1206: False},\n",
       " 'pwls': {1206: 6},\n",
       " 'retrieved_on': {1206: 1614410336},\n",
       " 'score': {1206: 327},\n",
       " 'selftext': {1206: nan},\n",
       " 'send_replies': {1206: False},\n",
       " 'spoiler': {1206: False},\n",
       " 'stickied': {1206: False},\n",
       " 'subreddit': {1206: 'googlehome'},\n",
       " 'subreddit_id': {1206: 't5_3enp4'},\n",
       " 'subreddit_subscribers': {1206: 340471},\n",
       " 'subreddit_type': {1206: 'public'},\n",
       " 'thumbnail': {1206: 'https://b.thumbs.redditmedia.com/qjJiE-d1dLafh7Pgle_1X0ZGXCu76d0Wfm7NvSws0jM.jpg'},\n",
       " 'title': {1206: 'Google kills most of its Disney read-along books for Nest and Home'},\n",
       " 'total_awards_received': {1206: 1},\n",
       " 'treatment_tags': {1206: '[]'},\n",
       " 'upvote_ratio': {1206: 0.96},\n",
       " 'url': {1206: 'https://www.androidpolice.com/2021/02/25/google-kills-most-of-its-disney-read-along-books-for-nest-and-home/'},\n",
       " 'whitelist_status': {1206: 'all_ads'},\n",
       " 'wls': {1206: 6},\n",
       " 'link_flair_template_id': {1206: nan},\n",
       " 'link_flair_text': {1206: nan},\n",
       " 'post_hint': {1206: 'link'},\n",
       " 'preview': {1206: \"{'enabled': False, 'images': [{'id': '3nFPD7vK6fE8X-He3I_eZKm9BZZpiDDRLb6ZFGAqZLk', 'resolutions': [{'height': 60, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d1acfaf16dbe4c9bf431ed6d3d695d216af1745', 'width': 108}, {'height': 121, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab245e31e5551c41f20de8f65c4d5847432c88bc', 'width': 216}, {'height': 180, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a1c72a860a44e9cc61584bfc5b28e065a7368e2', 'width': 320}, {'height': 360, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=551ac44195c06943998ff08d10e9110c597b7f21', 'width': 640}, {'height': 540, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89c4a97c7be63fa8e3ab28e437a71653ec5154ce', 'width': 960}, {'height': 607, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4734f45b829540d71dd116d7badbda7b4c2be110', 'width': 1080}], 'source': {'height': 1440, 'url': 'https://external-preview.redd.it/5RiwvRpjAswcnxp3RvdFoM0sjTEX91qlEkwwSP45Yms.jpg?auto=webp&amp;s=9828da0dabd0557f9d631f9337a30c4812d1cd41', 'width': 2560}, 'variants': {}}]}\"},\n",
       " 'thumbnail_height': {1206: 78.0},\n",
       " 'thumbnail_width': {1206: 140.0},\n",
       " 'url_overridden_by_dest': {1206: 'https://www.androidpolice.com/2021/02/25/google-kills-most-of-its-disney-read-along-books-for-nest-and-home/'},\n",
       " 'removed_by_category': {1206: nan},\n",
       " 'author_flair_background_color': {1206: nan},\n",
       " 'author_flair_text_color': {1206: nan},\n",
       " 'media': {1206: nan},\n",
       " 'media_embed': {1206: nan},\n",
       " 'secure_media': {1206: nan},\n",
       " 'secure_media_embed': {1206: nan},\n",
       " 'gallery_data': {1206: nan},\n",
       " 'is_gallery': {1206: nan},\n",
       " 'media_metadata': {1206: nan},\n",
       " 'author_flair_template_id': {1206: nan},\n",
       " 'author_cakeday': {1206: nan},\n",
       " 'poll_data': {1206: nan},\n",
       " 'crosspost_parent': {1206: nan},\n",
       " 'crosspost_parent_list': {1206: nan},\n",
       " 'edited': {1206: nan},\n",
       " 'collections': {1206: nan},\n",
       " 'banned_by': {1206: nan},\n",
       " 'gilded': {1206: nan},\n",
       " 'distinguished': {1206: nan}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data dictionary of document with 221 comments\n",
    "\n",
    "ggl_title_only_df[ggl_title_only_df.num_comments == 221].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to title only post without a link_flair_text category, with highest number of comments: https://www.reddit.com/r/googlehome/comments/lsjp5n/google_kills_most_of_its_disney_readalong_books/\n"
     ]
    }
   ],
   "source": [
    "#print url of post for further investigation\n",
    "print (f\"Link to title only post without a link_flair_text category, with highest number of comments: {ggl_title_only_df[ggl_title_only_df.num_comments == 221].to_dict()['full_link'][1206]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>: Similar to the document which had high number of comments, this document had also gone 'viral'.\n",
    "    \n",
    "The title of this post being 'Google kills most of its Disney read-along books for Nest and Home'. This is news relating the product. There were no images and only a url.\n",
    "    \n",
    "This is another a good example of how just the heading alone conveys the overall sentiment of what the redditors wants to post without self.text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: from the above, it is safe to assume that for posts not having selftext, as well as a link_flair_text category, the titles more or less captures the sentiment or overall message that the redditor is espressing. Albeit some post could be peculiar or nonsensical yet still having something to do with the product.\n",
    "    \n",
    "Before deciding to transform documents with only titles (copying the words of the respective title to it being under the self.text column), I would like to know what is the frequency of the presense of a url or photo for these posts. This can be determine by the feature 'url_overridden_by_dest'. The presence of an image would have a value of \"https://i.imgur.com...\" whereas a link would have the respective link itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gauge percentage of title only documents with links or pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_flair_text\n",
       "Bug                            38\n",
       "Commands | How To's             5\n",
       "Deals | Sales | Promotions      7\n",
       "Features WishList               6\n",
       "GH Trigger Warning              2\n",
       "Hacks                           7\n",
       "Help                           90\n",
       "NSFW - Language                 7\n",
       "News                           16\n",
       "Other                          36\n",
       "Product Review                  5\n",
       "Tips                           16\n",
       "Name: url_overridden_by_dest, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggl_title_only_df.groupby(by='link_flair_text')['url_overridden_by_dest'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_flair_text\n",
       "Bug                            0.883721\n",
       "Commands | How To's            0.714286\n",
       "Deals | Sales | Promotions     1.000000\n",
       "Features WishList              0.750000\n",
       "GH Trigger Warning             0.666667\n",
       "Hacks                          0.875000\n",
       "Help                           0.720000\n",
       "NSFW - Language                0.875000\n",
       "News                           0.888889\n",
       "Other                          0.900000\n",
       "Product Review                 0.714286\n",
       "Tips                           0.800000\n",
       "Name: url_overridden_by_dest, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Use group by function to only show presence of \n",
    "url or image per link_flair_text category on Google Home df\"\"\"\n",
    "\n",
    "#count of post with url divide by total number of posts per link_flair_text\n",
    "ggl_title_only_df.groupby(by='link_flair_text')['url_overridden_by_dest'].count()/ggl_title_only_df.groupby(by='link_flair_text')['url_overridden_by_dest'].agg(lambda x: np.count_nonzero(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Google Home posts without selftext that has a url or image: 84.74945533769062%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of Google Home posts without selftext that has a url or image: {ggl_title_only_df.url_overridden_by_dest.count()/len(ggl_title_only_df)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_flair_text\n",
       "Alexa Skill        0.741935\n",
       "Easter Egg         0.000000\n",
       "Feature            0.906250\n",
       "Feature Request    0.500000\n",
       "Question           0.430380\n",
       "Review             0.805556\n",
       "Skill Request      0.000000\n",
       "Technical Issue    0.619048\n",
       "Name: url_overridden_by_dest, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Use group by function to only show presence of \n",
    "url or image per link_flair_text category on Amazon df\"\"\"\n",
    "\n",
    "#count of post with url divide by total number of posts per link_flair_text\n",
    "amz_title_only_df.groupby(by='link_flair_text')['url_overridden_by_dest'].count()/amz_title_only_df.groupby(by='link_flair_text')['url_overridden_by_dest'].agg(lambda x: np.count_nonzero(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Amazon Echo posts without self.text that has a url or image: 61.568627450980394%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of Amazon Echo posts without self.text that has a url or image: {amz_title_only_df.url_overridden_by_dest.count()/len(amz_title_only_df)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>: From the above we can see that more than 50 percent posts from both subreddits that had no selftext and only a title, had included either a link or an image. This confirmed my earlier assumption. \n",
    "\n",
    "Discovering this has prompted me to look further into the documents. I am curious to find out if there are any reoccuring urls, likely from authors promoting something. If so, these documents would have to be looked at and assessed if their posts are meaningful to our model, otherwise they would have to be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate for reoccuring urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "https://reviewsfund.com/product/buy-facebook-reviews/                              54\n",
       "https://reviewsfund.com/product/buy-trustpilot-reviews/                            53\n",
       "https://usareviewshop.com/product/buy-verified-paypal-accounts/                    46\n",
       "https://usareviewshop.com/product/buy-verified-cash-app-accounts/                  37\n",
       "https://usatopservices.com/product/buy-google-5-star-reviews/                      13\n",
       "https://usatopservices.com/product/buy-facebook-page-likes/                        10\n",
       "https://reviewsfund.com/product/buy-sitejabber-reviews/                             7\n",
       "https://tiktokvpnforindia.blogspot.com/2021/02/android-best-ripple-vpn-app.html     5\n",
       "https://usatopsmm.com/product/buy-stripe-account/                                   4\n",
       "https://www.reddit.com/                                                             4\n",
       "Name: url_overridden_by_dest, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for top 10 common urls in Google Home ddf\n",
    "ggl_df.url_overridden_by_dest.value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings on posts with url: https://reviewsfund.com/product/buy-facebook-reviews/\n",
      "Number of posts with url: 54\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Facebook Reviews - USA Facebook Page Positive Reviews Rating']\n",
      "Total unique authors: 54\n",
      "\n",
      "\n",
      "Findings on posts with url: https://reviewsfund.com/product/buy-trustpilot-reviews/\n",
      "Number of posts with url: 53\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Trustpilot Reviews - 100% USA, UK, AU Permanent Trustpilot Reviews'\n",
      " 'Buy Trustpilot Reviews - 100% USA, UK, AU Permanen Trustpilot Reviews'\n",
      " 'Buy Trustpilot Reviews - 100% Permanent USA, UK, AU Reviews']\n",
      "Total unique authors: 53\n",
      "\n",
      "\n",
      "Findings on posts with url: https://usareviewshop.com/product/buy-verified-paypal-accounts/\n",
      "Number of posts with url: 46\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Verified PayPal Accounts - USA Verified Personal &amp; Business Account']\n",
      "Total unique authors: 46\n",
      "\n",
      "\n",
      "Findings on posts with url: https://usareviewshop.com/product/buy-verified-cash-app-accounts/\n",
      "Number of posts with url: 37\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Verified Cash App Accounts - Full Verified Cash App Accounts']\n",
      "Total unique authors: 37\n",
      "\n",
      "\n",
      "Findings on posts with url: https://usatopservices.com/product/buy-google-5-star-reviews/\n",
      "Number of posts with url: 13\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Google 5 Star Reviews - Permanent 5 Star Reviews'\n",
      " 'Buy Google 5-Star Reviews - Parmanet 5-Star Reviews Service']\n",
      "Total unique authors: 13\n",
      "\n",
      "\n",
      "Findings on posts with url: https://usatopservices.com/product/buy-facebook-page-likes/\n",
      "Number of posts with url: 10\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Facebook Page Likes - Facebook Page Likes USA']\n",
      "Total unique authors: 10\n",
      "\n",
      "\n",
      "Findings on posts with url: https://reviewsfund.com/product/buy-sitejabber-reviews/\n",
      "Number of posts with url: 7\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Sitejabber Reviews - 100% Safe nondrop USA, UK sitejabber reviews']\n",
      "Total unique authors: 7\n",
      "\n",
      "\n",
      "Findings on posts with url: https://tiktokvpnforindia.blogspot.com/2021/02/android-best-ripple-vpn-app.html\n",
      "Number of posts with url: 5\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Android best ripple vpn app']\n",
      "Total unique authors: 5\n",
      "\n",
      "\n",
      "Findings on posts with url: https://www.reddit.com/\n",
      "Number of posts with url: 4\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['My routine is scheduled for 730 but it runs at 530. Why']\n",
      "Total unique authors: 4\n",
      "\n",
      "\n",
      "Findings on posts with url: https://usatopsmm.com/product/buy-stripe-account/\n",
      "Number of posts with url: 4\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Stripe Account - 100% Real Business Verified Stripe']\n",
      "Total unique authors: 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check title,selftext and authors of posts with reoccuring urls for further investigation\n",
    "\n",
    "common_ggl_url = ggl_df.url_overridden_by_dest.value_counts().head(10).index.to_list()\n",
    "\n",
    "#use for loop to iterate through different attributes\n",
    "for x in common_ggl_url:\n",
    "    print (f'Findings on posts with url: {x}')\n",
    "    print (f\"Number of posts with url: {len(ggl_df[ggl_df.url_overridden_by_dest == x])}\")\n",
    "    print(f\"Total number of selftext: {ggl_df[ggl_df.url_overridden_by_dest == x]['selftext'].sum()}\")\n",
    "    print(f\"Unique titles:\")\n",
    "    print(ggl_df[ggl_df.url_overridden_by_dest == x]['title'].unique())\n",
    "    print(f\"Total unique authors: {len(ggl_df[ggl_df.url_overridden_by_dest == x]['author'].values)}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "https://digitalservice24h.com/service/buy-amazon-accounts/                                 9\n",
       "https://cpavox.com/track/0463fe4fb8e                                                       7\n",
       "http://worldquizzes.com                                                                    3\n",
       "http://sweepstakes2021.com                                                                 2\n",
       "https://zasnewsshshidri.blogspot.com/2020/12/sense-energy-monitor-with-solar-track.html    2\n",
       "https://direct-link.net/221756/activation                                                  2\n",
       "https://www.thesecretline.xyz/2021/01/amazon-offering-up-to-56-off-on-several.html         2\n",
       "http://worldinfo.tn/molicui-vertical-charging-stand-compatible-with-sony-ps5/              2\n",
       "http://smarthome.soulsprawl.com/find-out-products-what-you/#smarthome                      2\n",
       "https://www.reddit.com/gallery/kx58d2                                                      1\n",
       "Name: url_overridden_by_dest, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for top 10 common urls in Amazon Echo ddf\n",
    "amz_df.url_overridden_by_dest.value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings on posts with url: https://digitalservice24h.com/service/buy-amazon-accounts/\n",
      "Number of posts with url: 9\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Buy Amazon Accounts' 'Buy Amazon Accounts - Buy Old Amazon Accounts'\n",
      " 'Buy Amazon Accounts - Buy Email Verified Amazon Accounts']\n",
      "Total unique authors: 9\n",
      "\n",
      "\n",
      "Findings on posts with url: https://cpavox.com/track/0463fe4fb8e\n",
      "Number of posts with url: 7\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Earn Free Amazon Gift Cards &amp; Codes Legally - GiftsJunkie']\n",
      "Total unique authors: 7\n",
      "\n",
      "\n",
      "Findings on posts with url: http://worldquizzes.com\n",
      "Number of posts with url: 3\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['All-new Echo Show 10 (3rd Gen) , Buy 2, save $100 off - today on Amazon'\n",
      " \"40% off All-new Echo Dot (4th Gen) + Amazon Smart Plug, today's deal\"\n",
      " '68% off Outlet Wall Mount Solution for Echo Dot Gen 2 - on Amazon today']\n",
      "Total unique authors: 3\n",
      "\n",
      "\n",
      "Findings on posts with url: http://sweepstakes2021.com\n",
      "Number of posts with url: 2\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "[\"40% off All-new Echo Dot (4th Gen) + Amazon Smart Plug, today's deal\"\n",
      " 'Black Friday deals, up tp 60% off Amazon devices']\n",
      "Total unique authors: 2\n",
      "\n",
      "\n",
      "Findings on posts with url: https://zasnewsshshidri.blogspot.com/2020/12/sense-energy-monitor-with-solar-track.html\n",
      "Number of posts with url: 2\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Sense Energy Monitor with Solar  Track Electricity Usage and Solar Production in Real Time and Save Money']\n",
      "Total unique authors: 2\n",
      "\n",
      "\n",
      "Findings on posts with url: https://direct-link.net/221756/activation\n",
      "Number of posts with url: 2\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['I got ya homie!']\n",
      "Total unique authors: 2\n",
      "\n",
      "\n",
      "Findings on posts with url: https://www.thesecretline.xyz/2021/01/amazon-offering-up-to-56-off-on-several.html\n",
      "Number of posts with url: 2\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Amazon offering up to 56% off on several Echo Show 5 &amp; Echo Show 8 bundles']\n",
      "Total unique authors: 2\n",
      "\n",
      "\n",
      "Findings on posts with url: http://worldinfo.tn/molicui-vertical-charging-stand-compatible-with-sony-ps5/\n",
      "Number of posts with url: 2\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['MOLICUI Vertical Charging Stand Compatible with Sony PS5']\n",
      "Total unique authors: 2\n",
      "\n",
      "\n",
      "Findings on posts with url: http://smarthome.soulsprawl.com/find-out-products-what-you/#smarthome\n",
      "Number of posts with url: 2\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['23% off Echo Show 8 HD smart display with Alexa stay connected with video calling on Amazon today']\n",
      "Total unique authors: 2\n",
      "\n",
      "\n",
      "Findings on posts with url: https://www.reddit.com/gallery/kx58d2\n",
      "Number of posts with url: 1\n",
      "Total number of selftext: 0\n",
      "Unique titles:\n",
      "['Energy dashboard appears in my US iPhone Alexa app under the device page']\n",
      "Total unique authors: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check title, selftext and authors of posts with reoccuring urls for further investigation\n",
    "\n",
    "common_amz_url = amz_df.url_overridden_by_dest.value_counts().sort_values(ascending=False).head(10).index.to_list()\n",
    "\n",
    "#use for loop to iterate through different attributes\n",
    "for x in common_amz_url:\n",
    "    print (f'Findings on posts with url: {x}')\n",
    "    print (f\"Number of posts with url: {len(amz_df[amz_df.url_overridden_by_dest == x])}\")\n",
    "    print(f\"Total number of selftext: {amz_df[amz_df.url_overridden_by_dest == x]['selftext'].sum()}\")\n",
    "    print(f\"Unique titles:\")\n",
    "    print(amz_df[amz_df.url_overridden_by_dest == x]['title'].unique())\n",
    "    print(f\"Total unique authors: {len(amz_df[amz_df.url_overridden_by_dest == x]['author'].values)}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.77% of the Google Home subreddit had reocuring urls\n",
      "1.07% of the Amazon Echo subreddit had reocuring urls\n"
     ]
    }
   ],
   "source": [
    "# find % of subreddit df that had reoccuring urls\n",
    "\n",
    "print (f'{round(ggl_df.url_overridden_by_dest.value_counts().head(10).sum()/len(ggl_df)*100,2)}% of the Google Home subreddit had reocuring urls')\n",
    "print (f'{round(amz_df.url_overridden_by_dest.value_counts().head(10).sum()/len(amz_df)*100,2)}% of the Amazon Echo subreddit had reocuring urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>: From the findings above, top 10 reoccuring urls posts, do not have selftext and some had variations of the same title. These post were all made by unique authors. Majority of these urls are promotional but are leaning towards the definition of 'spam' or even phishing sites. \n",
    "\n",
    "Another observation is that the Google Home subreddit had approximately 7 times more of these type of urls compared to the Amazon home subreddit from a sample of 3000 posts each.\n",
    "\n",
    "Moving forward, these documents would be removed from their respective dataframes as they are infact not meaningful for out predictions but these findings are worth mentioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Home df shape:(3000, 85)\n",
      "Amazon Echo df shape:(3000, 83)\n"
     ]
    }
   ],
   "source": [
    "print(f'Google Home df shape:{ggl_df.shape}')\n",
    "print(f'Amazon Echo df shape:{amz_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate posts made by same authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to uncovering interesting findings with reoccuring urls, this section will look into multiple posts made by the same authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[deleted]          31\n",
       "halime123          27\n",
       "oyirinnayaa        23\n",
       "2tuff4u2            9\n",
       "johnkhoo            9\n",
       "monicakmtx          9\n",
       "HeyCharrrrlie       7\n",
       "DerrickWolfesmm     7\n",
       "Newwales2           6\n",
       "newyerker           6\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show top 10 Google Home subreddit authors\n",
    "\n",
    "ggl_df.author.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>url_overridden_by_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Where the fuck is the \"routines\" option in the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Just got a 2nd gen Nest Hub. A couple of thing...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Removed Nest Mini from Home app but now I can'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Emergency Calls from Nest Mini for Children</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Help! I cannot connect to the Gosung app.</td>\n",
       "      <td>https://v.redd.it/35r1uqq97vk61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Leaked photos of the new Nest Hub Bastard</td>\n",
       "      <td>https://www.reddit.com/gallery/ltnubf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Chromecast connection issue</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>New Google Nest Hub</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Google mini just announced \"someone is at your...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Voice Match states \"Trouble Connecting\" when I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Cant get the time to change to matter what I ...</td>\n",
       "      <td>https://i.redd.it/ypjd5l2icog61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Lenovo Smart Clock 5GHz Problem</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Google home voice intermittently doesn't want ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Is there a way to send a message to a specific...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Google Assistant can't read Outlook events</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I asked Google to play classical music. Google...</td>\n",
       "      <td>https://imgur.com/a/pkpy1rP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Speaker Groups Gone?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Sorry, it looks like that device hasnt been ...</td>\n",
       "      <td>https://i.redd.it/ql063oq6m5f61.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Google assistant won't sync devices anymore! C...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>It looks like that device hasn't been set up y...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Google Home completely failed today</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C by GE bulbs no longer available to control b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Voice train in different languages for each ho...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Ive set my routine to start on my bedroom spe...</td>\n",
       "      <td>https://www.reddit.com/gallery/layz97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Does anyone know where routines button moved to?</td>\n",
       "      <td>https://www.reddit.com/gallery/laq5v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Guys I Want To Increase My Lenovo Tab 7 Eassan...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Apple Music</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Can I turn on my PC with google home and a IOS...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Can someone play Puzzle Of The Day and tell me...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Sony speakers not showing up in notification c...</td>\n",
       "      <td>https://www.reddit.com/gallery/l99bgi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Please help! My plugs are connected to the Sma...</td>\n",
       "      <td>https://v.redd.it/acwla7qedsb61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       selftext                                              title  \\\n",
       "293   [deleted]  Where the fuck is the \"routines\" option in the...   \n",
       "301   [deleted]  Just got a 2nd gen Nest Hub. A couple of thing...   \n",
       "303   [deleted]  Removed Nest Mini from Home app but now I can'...   \n",
       "1063  [deleted]        Emergency Calls from Nest Mini for Children   \n",
       "1069  [deleted]          Help! I cannot connect to the Gosung app.   \n",
       "1151  [deleted]          Leaked photos of the new Nest Hub Bastard   \n",
       "1172        NaN                        Chromecast connection issue   \n",
       "1209  [deleted]                                New Google Nest Hub   \n",
       "1249  [deleted]  Google mini just announced \"someone is at your...   \n",
       "1283  [deleted]  Voice Match states \"Trouble Connecting\" when I...   \n",
       "1772  [deleted]  Cant get the time to change to matter what I ...   \n",
       "1864  [deleted]                    Lenovo Smart Clock 5GHz Problem   \n",
       "1873  [deleted]  Google home voice intermittently doesn't want ...   \n",
       "1917  [deleted]  Is there a way to send a message to a specific...   \n",
       "1918  [deleted]         Google Assistant can't read Outlook events   \n",
       "1924  [deleted]  I asked Google to play classical music. Google...   \n",
       "1941  [deleted]                               Speaker Groups Gone?   \n",
       "1956  [deleted]  Sorry, it looks like that device hasnt been ...   \n",
       "1962        NaN  Google assistant won't sync devices anymore! C...   \n",
       "1965  [deleted]  It looks like that device hasn't been set up y...   \n",
       "1966        NaN                Google Home completely failed today   \n",
       "1972        NaN  C by GE bulbs no longer available to control b...   \n",
       "1975  [deleted]  Voice train in different languages for each ho...   \n",
       "1997  [deleted]  Ive set my routine to start on my bedroom spe...   \n",
       "2009  [deleted]   Does anyone know where routines button moved to?   \n",
       "2011        NaN  Guys I Want To Increase My Lenovo Tab 7 Eassan...   \n",
       "2027  [deleted]                                        Apple Music   \n",
       "2078  [deleted]  Can I turn on my PC with google home and a IOS...   \n",
       "2079  [deleted]  Can someone play Puzzle Of The Day and tell me...   \n",
       "2099  [deleted]  Sony speakers not showing up in notification c...   \n",
       "2659  [deleted]  Please help! My plugs are connected to the Sma...   \n",
       "\n",
       "                     url_overridden_by_dest  \n",
       "293                                     NaN  \n",
       "301                                     NaN  \n",
       "303                                     NaN  \n",
       "1063                                    NaN  \n",
       "1069        https://v.redd.it/35r1uqq97vk61  \n",
       "1151  https://www.reddit.com/gallery/ltnubf  \n",
       "1172                                    NaN  \n",
       "1209                                    NaN  \n",
       "1249                                    NaN  \n",
       "1283                                    NaN  \n",
       "1772    https://i.redd.it/ypjd5l2icog61.jpg  \n",
       "1864                                    NaN  \n",
       "1873                                    NaN  \n",
       "1917                                    NaN  \n",
       "1918                                    NaN  \n",
       "1924            https://imgur.com/a/pkpy1rP  \n",
       "1941                                    NaN  \n",
       "1956    https://i.redd.it/ql063oq6m5f61.jpg  \n",
       "1962                                    NaN  \n",
       "1965                                    NaN  \n",
       "1966                                    NaN  \n",
       "1972                                    NaN  \n",
       "1975                                    NaN  \n",
       "1997  https://www.reddit.com/gallery/layz97  \n",
       "2009  https://www.reddit.com/gallery/laq5v1  \n",
       "2011                                    NaN  \n",
       "2027                                    NaN  \n",
       "2078                                    NaN  \n",
       "2079                                    NaN  \n",
       "2099  https://www.reddit.com/gallery/l99bgi  \n",
       "2659        https://v.redd.it/acwla7qedsb61  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check '[deleted]' author against text title,and url\n",
    "\n",
    "ggl_df[ggl_df.author == '[deleted]'][['selftext','title','url_overridden_by_dest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kindly_Baby4695         28\n",
       "[deleted]               25\n",
       "Secure-Quality-6516     20\n",
       "RamITT                  10\n",
       "IfuDidntCome2Party      10\n",
       "Legend1138               8\n",
       "redwingshat              8\n",
       "AXXXXXXXXA               8\n",
       "_BindersFullOfWomen_     7\n",
       "Bakura_1993              7\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show top 10 Amazon Echo subreddit authors\n",
    "\n",
    "amz_df.author.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>url_overridden_by_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Rending and the Nest Hardcover</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/the-rending...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Logitech G502 Hero High Performance Gaming Mouse</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/logitech-g5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8\" LED Selfie Ring Light for Live Stream/Makeu...</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/8-led-selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Linenspa All-Season White Down Alternative Qui...</td>\n",
       "      <td>https://youtube.com/watch?v=MrikkijE-LA&amp;amp;fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Linenspa All-Season White Down Alternative Qui...</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/linenspa-al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Acer Aspire 5 Slim Laptop, 15.6 inches Full HD...</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/acer-aspire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sperry Instruments STK001 Non-Contact Voltage ...</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/sperry-inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple AirPods with Charging Case (Wired)</td>\n",
       "      <td>https://www.shahidriaz.com/2021/01/apple-airpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Belkin 12-Outlet Power Strip Surge Protector, ...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/belkin-12-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Etekcity Digital Body Weight Bathroom Scale wi...</td>\n",
       "      <td>https://youtube.com/watch?v=mac4KPW3e9A&amp;amp;fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Etekcity Digital Body Weight Bathroom Scale wi...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/etekcity-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Muslin Burp Cloths 6 Pack Large 100% Cotton Ha...</td>\n",
       "      <td>https://youtube.com/watch?v=V7Ko5NiHeOM&amp;amp;fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Muslin Burp Cloths 6 Pack Large 100% Cotton Ha...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/muslin-burp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sense Energy Monitor with Solar  Track Electr...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/blog-post_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bounty Quick-Size Paper Towels, White, 16 Fami...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/bounty-quic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>NaN</td>\n",
       "      <td>essence | Lash Princess False Lash Effect Masc...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/essence-las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Baby Wipes, Pampers Sensitive Water Based Baby...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/baby-wipes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Nixplay Smart Digital Picture Frame 10.1 Inch,...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/nixplay-sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Rubbermaid Easy Find Vented Lids Food Storage ...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/rubbermaid-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Aquaphor Healing Ointment - Moisturizing Skin ...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/aquaphor-he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Massage Gun Deep Tissue Percussion Muscle Mass...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/massage-gun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Lenovo Tab M10 Plus, 10.3\" FHD Android Tablet,...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/lenovo-tab-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Motorola Video Baby Monitor 5Color Parent Uni...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/motorola-vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>All-new Echo Dot (4th Gen) with clock - Twilig...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/all-new-ech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Turtle Beach Headset Audio Controller Plus for...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/turtle-beac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>HP Stream 11.6-inch HD Laptop, Intel Celeron N...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/blog-post_9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Kaisi 136 in 1 Electronics Repair Tool Kit Pro...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/kaisi-136-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Massage Gun Deep Tissue Percussion Muscle Mass...</td>\n",
       "      <td>https://www.shahidriaz.com/2020/12/massage-gun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     selftext                                              title  \\\n",
       "1006      NaN                 The Rending and the Nest Hardcover   \n",
       "1437      NaN   Logitech G502 Hero High Performance Gaming Mouse   \n",
       "1451      NaN  8\" LED Selfie Ring Light for Live Stream/Makeu...   \n",
       "1452      NaN  Linenspa All-Season White Down Alternative Qui...   \n",
       "1455      NaN  Linenspa All-Season White Down Alternative Qui...   \n",
       "1457      NaN  Acer Aspire 5 Slim Laptop, 15.6 inches Full HD...   \n",
       "1460      NaN  Sperry Instruments STK001 Non-Contact Voltage ...   \n",
       "1464      NaN           Apple AirPods with Charging Case (Wired)   \n",
       "1486      NaN  Belkin 12-Outlet Power Strip Surge Protector, ...   \n",
       "1489      NaN  Etekcity Digital Body Weight Bathroom Scale wi...   \n",
       "1490      NaN  Etekcity Digital Body Weight Bathroom Scale wi...   \n",
       "1493      NaN  Muslin Burp Cloths 6 Pack Large 100% Cotton Ha...   \n",
       "1494      NaN  Muslin Burp Cloths 6 Pack Large 100% Cotton Ha...   \n",
       "1499      NaN  Sense Energy Monitor with Solar  Track Electr...   \n",
       "1503      NaN  Bounty Quick-Size Paper Towels, White, 16 Fami...   \n",
       "1520      NaN  essence | Lash Princess False Lash Effect Masc...   \n",
       "1524      NaN  Baby Wipes, Pampers Sensitive Water Based Baby...   \n",
       "1527      NaN  Nixplay Smart Digital Picture Frame 10.1 Inch,...   \n",
       "1530      NaN  Rubbermaid Easy Find Vented Lids Food Storage ...   \n",
       "1532      NaN  Aquaphor Healing Ointment - Moisturizing Skin ...   \n",
       "1562      NaN  Massage Gun Deep Tissue Percussion Muscle Mass...   \n",
       "1564      NaN  Lenovo Tab M10 Plus, 10.3\" FHD Android Tablet,...   \n",
       "1567      NaN  Motorola Video Baby Monitor 5Color Parent Uni...   \n",
       "1570      NaN  All-new Echo Dot (4th Gen) with clock - Twilig...   \n",
       "1572      NaN  Turtle Beach Headset Audio Controller Plus for...   \n",
       "1574      NaN  HP Stream 11.6-inch HD Laptop, Intel Celeron N...   \n",
       "1575      NaN  Kaisi 136 in 1 Electronics Repair Tool Kit Pro...   \n",
       "1708      NaN  Massage Gun Deep Tissue Percussion Muscle Mass...   \n",
       "\n",
       "                                 url_overridden_by_dest  \n",
       "1006  https://www.shahidriaz.com/2021/01/the-rending...  \n",
       "1437  https://www.shahidriaz.com/2021/01/logitech-g5...  \n",
       "1451  https://www.shahidriaz.com/2021/01/8-led-selfi...  \n",
       "1452  https://youtube.com/watch?v=MrikkijE-LA&amp;fe...  \n",
       "1455  https://www.shahidriaz.com/2021/01/linenspa-al...  \n",
       "1457  https://www.shahidriaz.com/2021/01/acer-aspire...  \n",
       "1460  https://www.shahidriaz.com/2021/01/sperry-inst...  \n",
       "1464  https://www.shahidriaz.com/2021/01/apple-airpo...  \n",
       "1486  https://www.shahidriaz.com/2020/12/belkin-12-o...  \n",
       "1489  https://youtube.com/watch?v=mac4KPW3e9A&amp;fe...  \n",
       "1490  https://www.shahidriaz.com/2020/12/etekcity-di...  \n",
       "1493  https://youtube.com/watch?v=V7Ko5NiHeOM&amp;fe...  \n",
       "1494  https://www.shahidriaz.com/2020/12/muslin-burp...  \n",
       "1499  https://www.shahidriaz.com/2020/12/blog-post_3...  \n",
       "1503  https://www.shahidriaz.com/2020/12/bounty-quic...  \n",
       "1520  https://www.shahidriaz.com/2020/12/essence-las...  \n",
       "1524  https://www.shahidriaz.com/2020/12/baby-wipes-...  \n",
       "1527  https://www.shahidriaz.com/2020/12/nixplay-sma...  \n",
       "1530  https://www.shahidriaz.com/2020/12/rubbermaid-...  \n",
       "1532  https://www.shahidriaz.com/2020/12/aquaphor-he...  \n",
       "1562  https://www.shahidriaz.com/2020/12/massage-gun...  \n",
       "1564  https://www.shahidriaz.com/2020/12/lenovo-tab-...  \n",
       "1567  https://www.shahidriaz.com/2020/12/motorola-vi...  \n",
       "1570  https://www.shahidriaz.com/2020/12/all-new-ech...  \n",
       "1572  https://www.shahidriaz.com/2020/12/turtle-beac...  \n",
       "1574  https://www.shahidriaz.com/2020/12/blog-post_9...  \n",
       "1575  https://www.shahidriaz.com/2020/12/kaisi-136-i...  \n",
       "1708  https://www.shahidriaz.com/2020/12/massage-gun...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 'Kindly_baby4695' author against text,title and url\n",
    "\n",
    "amz_df[amz_df.author == 'Kindly_Baby4695'][['selftext','title','url_overridden_by_dest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[deleted]               31\n",
       "halime123               27\n",
       "oyirinnayaa             23\n",
       "2tuff4u2                 9\n",
       "johnkhoo                 9\n",
       "monicakmtx               9\n",
       "HeyCharrrrlie            7\n",
       "DerrickWolfesmm          7\n",
       "Regular_Raspberry_48     6\n",
       "thirteen_20              6\n",
       "PrestigiousBell8866      6\n",
       "Newwales2                6\n",
       "Kinglens311              6\n",
       "newyerker                6\n",
       "lukusw78                 5\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggl_df.author.value_counts().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author name: 855\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 385\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1398\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 2118\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1726\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 184\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 452\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1928\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1824\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 2169\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-25fb40a02a24>:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  print (f\"Percetage of urls to posts: {round(ggl_df[ggl_df.author == x]['url_overridden_by_dest'].notnull().sum()/len(ggl_df[ggl_df.author == x]['selftext'])*100,2)}%\")\n"
     ]
    }
   ],
   "source": [
    "#print findings of top 10 authors in Google Home subreddit\n",
    "\n",
    "ggl_top_authors = ggl_df.author.sort_values(ascending=False).head(10).index.to_list() #make top authors into list\n",
    "\n",
    "for x in ggl_top_authors:\n",
    "    print (f'Author name: {x}')\n",
    "    print (f\"Number of posts: {len(ggl_df[ggl_df.author == x]['selftext'])}\") #print number of posts\n",
    "    print (f\"Number of urls: {(ggl_df[ggl_df.author == x]['url_overridden_by_dest'].notnull()).sum()}\") #print number of urls to compare with number of posts\n",
    "    print (f\"Percetage of urls to posts: {round(ggl_df[ggl_df.author == x]['url_overridden_by_dest'].notnull().sum()/len(ggl_df[ggl_df.author == x]['selftext'])*100,2)}%\")\n",
    "    print ('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[855, 385, 1398, 2118, 1726, 184, 452, 1928, 1824, 2169]\n"
     ]
    }
   ],
   "source": [
    "print(ggl_top_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author name: 52\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1356\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 737\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 262\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 2490\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 720\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1786\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 2182\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 1883\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n",
      "Author name: 589\n",
      "Number of posts: 0\n",
      "Number of urls: 0\n",
      "Percetage of urls to posts: nan%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-686683d27424>:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  print (f\"Percetage of urls to posts: {round(amz_df[amz_df.author == x]['url_overridden_by_dest'].notnull().sum()/len(amz_df[amz_df.author == x]['selftext'])*100,2)}%\")\n"
     ]
    }
   ],
   "source": [
    "#print findings of top 10 authors in Amazon Echo subreddit\n",
    "\n",
    "amz_top_authors = amz_df.author.sort_values(ascending=False).head(10).index.to_list() #make top authors into list\n",
    "\n",
    "for x in amz_top_authors:\n",
    "    print (f'Author name: {x}')\n",
    "    print (f\"Number of posts: {len(amz_df[amz_df.author == x]['selftext'])}\") #print number of posts\n",
    "    print (f\"Number of urls: {amz_df[amz_df.author == x]['url_overridden_by_dest'].notnull().sum()}\") #print number of urls to compare with number of posts\n",
    "    print (f\"Percetage of urls to posts: {round(amz_df[amz_df.author == x]['url_overridden_by_dest'].notnull().sum()/len(amz_df[amz_df.author == x]['selftext'])*100,2)}%\")\n",
    "    print ('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-7c92ae6e6f65>:8: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if (ggl_df[ggl_df.author == x]['url_overridden_by_dest'].notnull().sum())/len(ggl_df[ggl_df.author == x]['selftext']) > 0.5:\n",
      "<ipython-input-43-7c92ae6e6f65>:15: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if (amz_df[amz_df.author == x]['url_overridden_by_dest'].notnull().sum())/len(amz_df[amz_df.author == x]['selftext']) > 0.5:\n"
     ]
    }
   ],
   "source": [
    "#create lists of authors to drop if % of urls to post > 0.5\n",
    "\n",
    "# authors to drop in google df\n",
    "ggl_authors_to_drop = []\n",
    "\n",
    "for x in ggl_top_authors:\n",
    "    \n",
    "    if (ggl_df[ggl_df.author == x]['url_overridden_by_dest'].notnull().sum())/len(ggl_df[ggl_df.author == x]['selftext']) > 0.5:\n",
    "        ggl_authors_to_drop.append(x)                                                                                                                                                              \n",
    "\n",
    "# authors to drop in amazon df\n",
    "amz_authors_to_drop = []\n",
    "\n",
    "for x in amz_top_authors:\n",
    "    if (amz_df[amz_df.author == x]['url_overridden_by_dest'].notnull().sum())/len(amz_df[amz_df.author == x]['selftext']) > 0.5:\n",
    "        amz_authors_to_drop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>: Both subreddits had authors named '[deleted]' as authors with high number posts. when measured against number of urls to number of posts, they scored a low percentage. Based on their titles, majority are troubleshooting related. Perhaps these authors had deleted their reddit account or had been removed by reddit for a variety of reasons.\n",
    "\n",
    "As for authors with high percentage of urls to post, these tend to be authors that are promoting a site or similar to the section above, spamming the subreddit.\n",
    "\n",
    "Moving forward, I will drop authors with high urls per post percentage above 50%, amongst the top 10 authors with multiple posts to ensure that the models are not affected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unwanted documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Home dataset shape before removing unwanted data: (3000, 85)\n",
      "Amazon dataset shape before removing unwanted data: (3000, 83)\n"
     ]
    }
   ],
   "source": [
    "#print dataframe shape prior to dropping unwanted data\n",
    "\n",
    "print (f'Google Home dataset shape before removing unwanted data: {ggl_df.shape}')\n",
    "print (f'Amazon dataset shape before removing unwanted data: {amz_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unwanted reoccuring url from respective dataset by index\n",
    "\n",
    "#create list of location index with reoccuring url in Google Home df\n",
    "ggl_index_to_drop = ggl_df[ggl_df['url_overridden_by_dest'].isin(common_ggl_url)].index.to_list()\n",
    "\n",
    "\n",
    "#create list of location index with authors to drop in Google Home df\n",
    "ggl_author_index_to_drop = ggl_df[ggl_df['author'].isin(ggl_authors_to_drop)].index.to_list()\n",
    "\n",
    "\n",
    "#create list of location index with reoccuring url in Amazon Echo df\n",
    "amz_index_to_drop = amz_df[amz_df['url_overridden_by_dest'].isin(common_amz_url)].index.to_list()\n",
    "\n",
    "\n",
    "#create list of location index with authors to drop in Amazon Echo df\n",
    "amz_author_index_to_drop = amz_df[amz_df['author'].isin(amz_authors_to_drop)].index.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to be deleted from Google Home df: 233\n",
      "Number of rows to be deleted from Amazon Echo df: 32\n"
     ]
    }
   ],
   "source": [
    "#Ensure there are no repeated index\n",
    "\n",
    "#final rows to delete from Google Home df\n",
    "ggl_index_to_drop_final = set(ggl_author_index_to_drop + ggl_index_to_drop)\n",
    "print(f'Number of rows to be deleted from Google Home df: {len(ggl_index_to_drop_final)}')\n",
    "\n",
    "#final rows to delete from Amazon Echo df\n",
    "amz_index_to_drop_final = set(amz_author_index_to_drop + amz_index_to_drop)\n",
    "print(f'Number of rows to be deleted from Amazon Echo df: {len(amz_index_to_drop_final)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Home dataset shape after removing unwanted data: (2767, 85)\n",
      "Amazon dataset shape after removing unwanted data: (2968, 83)\n"
     ]
    }
   ],
   "source": [
    "#Delete rows of unwanted data from respective df\n",
    "\n",
    "#Delete rows off Google Home df\n",
    "ggl_df.drop(index=ggl_index_to_drop_final, inplace=True)\n",
    "ggl_df.reset_index(inplace=True, drop=True) #reset df index and drop old index column\n",
    "\n",
    "#Delete rows off Amazon Echo df\n",
    "amz_df.drop(index=amz_index_to_drop_final, inplace=True)\n",
    "amz_df.reset_index(inplace=True, drop=True) #reset df index and drop old index column\n",
    "\n",
    "\n",
    "print (f'Google Home dataset shape after removing unwanted data: {ggl_df.shape}')\n",
    "print (f'Amazon dataset shape after removing unwanted data: {amz_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataframe shape: (5735, 87)\n"
     ]
    }
   ],
   "source": [
    "#concat subreddit df into on df\n",
    "\n",
    "joined_df = pd.concat([ggl_df,amz_df],axis=0)\n",
    "joined_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print(f'Combined dataframe shape: {joined_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selction and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5735 entries, 0 to 5734\n",
      "Data columns (total 87 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Unnamed: 0                     5735 non-null   int64  \n",
      " 1   all_awardings                  5735 non-null   object \n",
      " 2   allow_live_comments            5735 non-null   bool   \n",
      " 3   author                         5735 non-null   object \n",
      " 4   author_flair_css_class         5 non-null      object \n",
      " 5   author_flair_richtext          5679 non-null   object \n",
      " 6   author_flair_text              52 non-null     object \n",
      " 7   author_flair_type              5679 non-null   object \n",
      " 8   author_fullname                5679 non-null   object \n",
      " 9   author_patreon_flair           5679 non-null   object \n",
      " 10  author_premium                 5679 non-null   object \n",
      " 11  awarders                       5735 non-null   object \n",
      " 12  can_mod_post                   5735 non-null   bool   \n",
      " 13  contest_mode                   5735 non-null   bool   \n",
      " 14  created_utc                    5735 non-null   int64  \n",
      " 15  domain                         5735 non-null   object \n",
      " 16  full_link                      5735 non-null   object \n",
      " 17  gildings                       5735 non-null   object \n",
      " 18  id                             5735 non-null   object \n",
      " 19  is_crosspostable               5735 non-null   bool   \n",
      " 20  is_meta                        5735 non-null   bool   \n",
      " 21  is_original_content            5735 non-null   bool   \n",
      " 22  is_reddit_media_domain         5735 non-null   bool   \n",
      " 23  is_robot_indexable             5735 non-null   bool   \n",
      " 24  is_self                        5735 non-null   bool   \n",
      " 25  is_video                       5735 non-null   bool   \n",
      " 26  link_flair_background_color    363 non-null    object \n",
      " 27  link_flair_richtext            5735 non-null   object \n",
      " 28  link_flair_text_color          5735 non-null   object \n",
      " 29  link_flair_type                5735 non-null   object \n",
      " 30  locked                         5735 non-null   bool   \n",
      " 31  media_only                     5735 non-null   bool   \n",
      " 32  no_follow                      5735 non-null   bool   \n",
      " 33  num_comments                   5735 non-null   int64  \n",
      " 34  num_crossposts                 5735 non-null   int64  \n",
      " 35  over_18                        5735 non-null   bool   \n",
      " 36  parent_whitelist_status        5735 non-null   object \n",
      " 37  permalink                      5735 non-null   object \n",
      " 38  pinned                         5735 non-null   bool   \n",
      " 39  pwls                           5735 non-null   int64  \n",
      " 40  retrieved_on                   5735 non-null   int64  \n",
      " 41  score                          5735 non-null   int64  \n",
      " 42  selftext                       4631 non-null   object \n",
      " 43  send_replies                   5735 non-null   bool   \n",
      " 44  spoiler                        5735 non-null   bool   \n",
      " 45  stickied                       5735 non-null   bool   \n",
      " 46  subreddit                      5735 non-null   object \n",
      " 47  subreddit_id                   5735 non-null   object \n",
      " 48  subreddit_subscribers          5735 non-null   int64  \n",
      " 49  subreddit_type                 5735 non-null   object \n",
      " 50  thumbnail                      5735 non-null   object \n",
      " 51  title                          5735 non-null   object \n",
      " 52  total_awards_received          5735 non-null   int64  \n",
      " 53  treatment_tags                 5735 non-null   object \n",
      " 54  upvote_ratio                   5735 non-null   float64\n",
      " 55  url                            5735 non-null   object \n",
      " 56  whitelist_status               5735 non-null   object \n",
      " 57  wls                            5735 non-null   int64  \n",
      " 58  link_flair_template_id         2339 non-null   object \n",
      " 59  link_flair_text                3197 non-null   object \n",
      " 60  post_hint                      938 non-null    object \n",
      " 61  preview                        938 non-null    object \n",
      " 62  thumbnail_height               911 non-null    float64\n",
      " 63  thumbnail_width                911 non-null    float64\n",
      " 64  url_overridden_by_dest         987 non-null    object \n",
      " 65  removed_by_category            625 non-null    object \n",
      " 66  author_flair_background_color  27 non-null     object \n",
      " 67  author_flair_text_color        111 non-null    object \n",
      " 68  media                          119 non-null    object \n",
      " 69  media_embed                    98 non-null     object \n",
      " 70  secure_media                   119 non-null    object \n",
      " 71  secure_media_embed             98 non-null     object \n",
      " 72  gallery_data                   50 non-null     object \n",
      " 73  is_gallery                     68 non-null     object \n",
      " 74  media_metadata                 82 non-null     object \n",
      " 75  author_flair_template_id       46 non-null     object \n",
      " 76  author_cakeday                 19 non-null     object \n",
      " 77  poll_data                      16 non-null     object \n",
      " 78  crosspost_parent               55 non-null     object \n",
      " 79  crosspost_parent_list          55 non-null     object \n",
      " 80  edited                         65 non-null     float64\n",
      " 81  collections                    1 non-null      object \n",
      " 82  banned_by                      5 non-null      object \n",
      " 83  gilded                         1 non-null      float64\n",
      " 84  distinguished                  5 non-null      object \n",
      " 85  link_flair_css_class           1663 non-null   object \n",
      " 86  suggested_sort                 2968 non-null   object \n",
      "dtypes: bool(18), float64(5), int64(10), object(54)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#show columns of new df\n",
    "joined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t5_34em3    2968\n",
       "t5_3enp4    2767\n",
       "Name: subreddit_id, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check column for usefullness\n",
    "\n",
    "joined_df.subreddit_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>observation</b>: subreddit column only show two values, therfore not meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354       1\n",
       "275       1\n",
       "247       1\n",
       "221       1\n",
       "185       1\n",
       "       ... \n",
       "4       436\n",
       "3       483\n",
       "2       716\n",
       "1       687\n",
       "0      1482\n",
       "Name: num_comments, Length: 113, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shown distribution of number of comments \n",
    "joined_df.num_comments.value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/googlehome/comments/lb6ahl/post_your_comments_here_on_the_device_not_yet_set/']\n",
      "\n",
      "\n",
      "['https://www.reddit.com/r/googlehome/comments/lb6ahl/post_your_comments_here_on_the_device_not_yet_set/']\n"
     ]
    }
   ],
   "source": [
    "#check to see if url column and full link column returns same value\n",
    "\n",
    "print(joined_df[joined_df.num_comments==354]['url'].values)\n",
    "print('\\n')\n",
    "print(joined_df[joined_df.num_comments==354]['full_link'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that will be selected for futher exploration are stated in the table below.\n",
    "\n",
    "| Features to keep:      \t| Reason:                                            \t|\n",
    "|------------------------\t|----------------------------------------------------\t|\n",
    "| subreddit              \t| splits subreddit into r/GoogleHome or r/AmazonEcho \t|\n",
    "| author                 \t| to view distribution of post per author            \t|\n",
    "| full_link                 | contains link to reddit post                       \t|\n",
    "| num_comments           \t| to view distribution of comments per post          \t|\n",
    "| url_overridden_by_dest \t| contains url used in a post                        \t|\n",
    "| selftext               \t| contains text body of post                         \t|\n",
    "| title                  \t| contains title of post                             \t|\n",
    "| link_flair_text           | cointains type of post                                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with selected feature shape: (5735, 8)\n"
     ]
    }
   ],
   "source": [
    "#created df with selected features\n",
    "\n",
    "joined_df = joined_df[['subreddit','author','full_link','num_comments','url_overridden_by_dest','selftext','title','link_flair_text']]\n",
    "\n",
    "print(f'Dataframe with selected feature shape: {joined_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5735 entries, 0 to 5734\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   subreddit               5735 non-null   object\n",
      " 1   author                  5735 non-null   object\n",
      " 2   full_link               5735 non-null   object\n",
      " 3   num_comments            5735 non-null   int64 \n",
      " 4   url_overridden_by_dest  987 non-null    object\n",
      " 5   selftext                4631 non-null   object\n",
      " 6   title                   5735 non-null   object\n",
      " 7   link_flair_text         3197 non-null   object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 358.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#show columns and null values of new df\n",
    "joined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values in selftext, as discuss earlier in subsection [investigate on documents with missing selftext](#investigate-on-documents-with-missing-selftext), these are posts that have titles that could be in the form of question or announcement, accompanied by a link or image instead of text. \n",
    "\n",
    "These titles therefore are sufficient enough to encapsulate the overall sentiment or message of their post, and therefore as a form of feature engineering, post with titles only and no selftext, will have their titles be copied into the selftext column to replace the null values. After which, the dataframe is ready to be exported as clean data for futher exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[removed]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  238\n",
       "[deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   53\n",
       "Post your requested features here, and vote on those listed.\\n\\nYou're also welcome to post your own feature requests as separate posts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     7\n",
       "https://imgur.com/gallery/ppqexQy\\n\\nNoob question.  When you open the Alexa app, and review enabled Skills.  Some are listed as UPDATED.\\n\\nIs there anything that the end user is suppose to do to ensure update is active on the account?  \\n\\nSince it's all back end servers anyway, I would think it would install the UPDATE automatically.  Why does it advise the end user it was UPDATED of we do not need to do anything to start using the UPDATE?                                                                                                                                                               5\n",
       "Hello. A family member of mine is disabled and cannot see to use a normal phone, so I've been using the Alexa App to call his Echo Dot so all he has to do is say \"Alexa, answer\". Works great. However, I'd like other family members to be able to call his Echo Dot as well. As it stands now, he can call anybody, but can only get calls from me. How can I change this?\\n\\nTo phrase this another way, how can multiple Alexa App/smartphone users call a specific Echo device?\\n\\nThanks!                                                                                                                             3\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ... \n",
       "Is there any way to have the Google assistant speak the reminder content immediately. For me the reminders lose their value when they just become a push notification mixed in with the rest. I like having my phone on do not disturb but would appreciate my reminders being yelled at me like an assistant or overbearing mom would.                                                                                                                                                                                                                                                                                      1\n",
       "I've set up a new smart plug so that I can occasionally restart or reboot our WiFi if it seems to be having issues. \\n\\nI set up a routine so that it will turn off the plug, wait 30 seconds and then turn the plug back on. \\n\\nThe routine works perfectly if I manually run it from the Alexa app, but it won't work properly when actually speaking to the Alexa device. \\n\\nIf I set it to say 'Alexa, reboot the WiFi' it starts telling me about a Netgear skill instead, and if I set it as 'Alexa, restart the wifi' it just says \"Sorry, what device?\"\\n\\nAny ideas how to ignore the Netgear skill somehow?      1\n",
       "We have echo devices in most of the rooms in the house, but my wife literally only uses one. She has loads of playlists set up on Spotify and I have loads set up on amazon music. \\n\\nBasically, can we set something up so her device always uses Spotify as a default while the rest maintain prime music?                                                                                                                                                                                                                                                                                                                1\n",
       "The favourite radio station in our house is a station called \"Absolute Radio\". Up until a couple of days ago \"Hey Google play Absolute Radio\" used to work every time, but now it's started playing one of it's sister stations, Absolute 80's. Absolute Radio still exists in TuneIn, and our hub is detecting our input correctly, but it's deciding to play a station that's a partial match instead of a complete match.\\n\\nIs there any way of fixing this, or is this a Google/TuneIn issue that they need to fix?                                                                                                     1\n",
       "If so, then what cool things have you figured out?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
       "Name: selftext, Length: 4311, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for self text value_counts\n",
    "joined_df.selftext.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"write for loop function to replace empty values in selftext column\n",
    "with values from title column'\"\"\"\n",
    "\n",
    "for x in range(0,len(joined_df)):   #iterate through every row\n",
    "    if pd.isnull(joined_df.iloc[x,5]) or joined_df.iloc[x,5] == '[removed]' or  joined_df.iloc[x,5] == '[deleted]':     #check to see if value is null,'[deleted]' or '[removed]' in selftext column\n",
    "        joined_df.iloc[x,5] = joined_df.iloc[x,6]  #copy value in title column, replacing null value \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5735 entries, 0 to 5734\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   subreddit               5735 non-null   object\n",
      " 1   author                  5735 non-null   object\n",
      " 2   full_link               5735 non-null   object\n",
      " 3   num_comments            5735 non-null   int64 \n",
      " 4   url_overridden_by_dest  987 non-null    object\n",
      " 5   selftext                5735 non-null   object\n",
      " 6   title                   5735 non-null   object\n",
      " 7   link_flair_text         3197 non-null   object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 358.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#show columns and null values of feature engineered df\n",
    "\n",
    "joined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export dataset as clean data\n",
    "\n",
    "joined_df.to_csv(r'../datasets/clean_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
